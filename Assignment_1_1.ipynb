{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwwxx1pdaVQ5LcyZv/X7a3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tarane2028/ADS500B/blob/main/Assignment_1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L9JpF5PLSCHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1.1. Difference between Data Science and Data Mining\n",
        "\n",
        "Data Science:\n",
        "Data Science is a multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It encompasses various techniques from statistics, data analysis, machine learning, and domain knowledge to analyze and interpret complex data.\n",
        "\n",
        "Data Mining:\n",
        "Data Mining is a subset of Data Science that focuses on discovering patterns, correlations, and anomalies in large datasets. It involves methods at the intersection of machine learning, statistics, and database systems to extract useful information from data.\n",
        "\n",
        "1.1.2. Difference between AI, Machine Learning, and Deep Learning\n",
        "\n",
        "Artificial Intelligence (AI):\n",
        "AI is a broad field of computer science focused on creating systems capable of performing tasks that typically require human intelligence. These tasks include reasoning, learning, problem-solving, perception, language understanding, and more.\n",
        "\n",
        "Machine Learning (ML):\n",
        "ML is a subset of AI that involves the use of algorithms and statistical models to enable computers to improve their performance on a specific task through experience. ML algorithms learn patterns from data and make predictions or decisions without being explicitly programmed.\n",
        "\n",
        "Deep Learning (DL):\n",
        "DL is a specialized subset of ML that uses neural networks with many layers (deep neural networks) to model and understand complex patterns in data. DL excels in tasks like image and speech recognition, natural language processing, and more, due to its ability to learn hierarchical representations of data.\n",
        "\n",
        "1.1.3. Difference between Supervised and Unsupervised Learning\n",
        "\n",
        "Supervised Learning:\n",
        "Supervised learning involves training a model on labeled data, where the input data is paired with the correct output. The model learns to make predictions or decisions based on this training data. An example is a spam email classifier that is trained on emails labeled as \"spam\" or \"not spam.\"\n",
        "\n",
        "Unsupervised Learning:\n",
        "Unsupervised learning involves training a model on unlabeled data, where the algorithm tries to identify patterns and relationships within the data without predefined labels. An example is customer segmentation, where the goal is to group customers based on purchasing behavior without prior knowledge of the group labels."
      ],
      "metadata": {
        "id": "BaXNHMOrkuVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load dataset\n",
        "data_url = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\"\n",
        "data = pd.read_csv(data_url)\n",
        "\n",
        "# Exploratory Data Analysis\n",
        "print(data.head())\n",
        "print(data.describe())\n",
        "print(data.info())\n",
        "\n",
        "# Feature engineering: adding new features\n",
        "data['rooms_per_household'] = data['total_rooms'] / data['households']\n",
        "data['bedrooms_per_room'] = data['total_bedrooms'] / data['total_rooms']\n",
        "data['population_per_household'] = data['population'] / data['households']\n",
        "\n",
        "# Prepare the data for Machine Learning algorithms\n",
        "X = data.drop(\"median_house_value\", axis=1)\n",
        "y = data[\"median_house_value\"].copy()\n",
        "\n",
        "# Data preprocessing: handling missing values\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "X_num = X.select_dtypes(include=[np.number])\n",
        "X_num_imputed = pd.DataFrame(imputer.fit_transform(X_num), columns=X_num.columns)\n",
        "\n",
        "# Convert categorical attribute \"ocean_proximity\" to one-hot vectors\n",
        "X_cat = pd.get_dummies(X[\"ocean_proximity\"], drop_first=True)\n",
        "\n",
        "# Combine numerical and categorical features\n",
        "X_prepared = pd.concat([X_num_imputed, X_cat], axis=1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_prepared, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f'Root Mean Squared Error: {rmse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pby62mWKkjDO",
        "outputId": "933b8ffb-9581-40e5-8c53-21eddb4c05d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
            "0    -122.23     37.88                41.0        880.0           129.0   \n",
            "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
            "2    -122.24     37.85                52.0       1467.0           190.0   \n",
            "3    -122.25     37.85                52.0       1274.0           235.0   \n",
            "4    -122.25     37.85                52.0       1627.0           280.0   \n",
            "\n",
            "   population  households  median_income  median_house_value ocean_proximity  \n",
            "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
            "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
            "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
            "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
            "4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n",
            "          longitude      latitude  housing_median_age   total_rooms  \\\n",
            "count  20640.000000  20640.000000        20640.000000  20640.000000   \n",
            "mean    -119.569704     35.631861           28.639486   2635.763081   \n",
            "std        2.003532      2.135952           12.585558   2181.615252   \n",
            "min     -124.350000     32.540000            1.000000      2.000000   \n",
            "25%     -121.800000     33.930000           18.000000   1447.750000   \n",
            "50%     -118.490000     34.260000           29.000000   2127.000000   \n",
            "75%     -118.010000     37.710000           37.000000   3148.000000   \n",
            "max     -114.310000     41.950000           52.000000  39320.000000   \n",
            "\n",
            "       total_bedrooms    population    households  median_income  \\\n",
            "count    20433.000000  20640.000000  20640.000000   20640.000000   \n",
            "mean       537.870553   1425.476744    499.539680       3.870671   \n",
            "std        421.385070   1132.462122    382.329753       1.899822   \n",
            "min          1.000000      3.000000      1.000000       0.499900   \n",
            "25%        296.000000    787.000000    280.000000       2.563400   \n",
            "50%        435.000000   1166.000000    409.000000       3.534800   \n",
            "75%        647.000000   1725.000000    605.000000       4.743250   \n",
            "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
            "\n",
            "       median_house_value  \n",
            "count        20640.000000  \n",
            "mean        206855.816909  \n",
            "std         115395.615874  \n",
            "min          14999.000000  \n",
            "25%         119600.000000  \n",
            "50%         179700.000000  \n",
            "75%         264725.000000  \n",
            "max         500001.000000  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20640 entries, 0 to 20639\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           20640 non-null  float64\n",
            " 1   latitude            20640 non-null  float64\n",
            " 2   housing_median_age  20640 non-null  float64\n",
            " 3   total_rooms         20640 non-null  float64\n",
            " 4   total_bedrooms      20433 non-null  float64\n",
            " 5   population          20640 non-null  float64\n",
            " 6   households          20640 non-null  float64\n",
            " 7   median_income       20640 non-null  float64\n",
            " 8   median_house_value  20640 non-null  float64\n",
            " 9   ocean_proximity     20640 non-null  object \n",
            "dtypes: float64(9), object(1)\n",
            "memory usage: 1.6+ MB\n",
            "None\n",
            "Root Mean Squared Error: 69126.81294720496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries such as NumPy, pandas, and scikit-learn are essential for data manipulation, model training, and evaluation. Matplotlib and seaborn are used for visualization.\n",
        "The dataset is loaded from a URL into a pandas DataFrame. This dataset contains various features related to housing.\n",
        "EDA helps in understanding the dataset by displaying the first few rows, statistical summary, and data types. This step is crucial for identifying any missing values or anomalies.\n",
        "New features are created from existing ones to help the model capture more relevant information. Examples include 'rooms_per_household' and 'bedrooms_per_room'.\n",
        "The target variable 'median_house_value' is separated from the features. Missing values in numerical features are handled using the SimpleImputer with a median strategy.\n",
        "The 'ocean_proximity' categorical attribute is converted to one-hot encoded vectors to be used in the model.\n",
        "Numerical and categorical features are combined into a single DataFrame for model training.\n",
        "The dataset is split into training and testing sets to evaluate the model's performance on unseen data.\n",
        "Features are scaled to have zero mean and unit variance, which helps many machine learning algorithms perform better.\n",
        "A linear regression model is trained on the scaled training data.\n",
        "The trained model is used to make predictions on the scaled test set. The root mean squared error (RMSE) is calculated to measure the model's performance. Lower RMSE indicates better model performance."
      ],
      "metadata": {
        "id": "p68WYzcflOHe"
      }
    }
  ]
}